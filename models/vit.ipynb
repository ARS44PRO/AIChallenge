{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Импорты\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib\n",
        "from pydub import AudioSegment\n",
        "import python_speech_features as psf\n",
        "from tensorflow.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import six\n",
        "import math\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, callbacks, models, callbacks\n",
        "from einops.layers.tensorflow import Rearrange\n",
        "\n",
        "%matplotlib inline\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1nPgcS8KkMAz"
      },
      "outputs": [],
      "source": [
        "# Константы\n",
        "DATA_PATH = '../new-data-without_silence'\n",
        "TRAIN_DATA_PATH = f'{DATA_PATH}/train'\n",
        "TASK_DATA_PATH = f'{DATA_PATH}/test'\n",
        "TRAIN_FILENAME = f'{DATA_PATH}/train_gt.csv'\n",
        "TASK_FILENAME = f'{DATA_PATH}/test.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igiJruf6kMA0",
        "outputId": "333919b7-6ae9-424c-92c0-8eb34f38d0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 0.7903573352486982, 1: 1.3610080395794681}\n"
          ]
        }
      ],
      "source": [
        "# Загрзка данных\n",
        "df = pd.read_csv(TRAIN_FILENAME, header=None, names=['audio', 'label'])\n",
        "df['audio'] = TRAIN_DATA_PATH + \"/\" + df['audio']\n",
        "\n",
        "x, y = df['audio'], df['label'].to_numpy().reshape(-1, 1)\n",
        "\n",
        "weight_for_0 = (1 / (len(y) - np.sum(y))) * (len(y) / 2.0)\n",
        "weight_for_1 = (1 / np.sum(y)) * (len(y) / 2.0)\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "print(class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WB16SNwlkMA1"
      },
      "outputs": [],
      "source": [
        "# # Обработка аудио\n",
        "\n",
        "# def preprocess_function(audio_path):\n",
        "#     segment = AudioSegment.from_mp3(audio_path)\n",
        "\n",
        "#     features, energy = psf.fbank(\n",
        "#         np.array(segment.get_array_of_samples()), segment.frame_rate\n",
        "#     )\n",
        "\n",
        "#     return features\n",
        "\n",
        "# features_list = []\n",
        "\n",
        "# for audio in tqdm(x):\n",
        "#     features_list.append(preprocess_function(audio))\n",
        "\n",
        "# import tensorflow as tf\n",
        "# X = tf.keras.utils.pad_sequences(features_list, padding='post', maxlen=4000)\n",
        "\n",
        "# np.save(\"train\", X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hz5tXg8pmrUC"
      },
      "outputs": [],
      "source": [
        "# Загрзка предпросчитанных (15-20 минут долго ждать, поэтому лучше так)\n",
        "X = np.load('train.npy').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Нормализация данных по лейблам ~50/50\n",
        "diff = len(y) - sum(y) - sum(y)\n",
        "\n",
        "indexes = []\n",
        "\n",
        "for ind, i in enumerate(y):\n",
        "    if i == 0:\n",
        "        diff -= 1\n",
        "        indexes.append(ind)\n",
        "        if diff == 0:\n",
        "            break\n",
        "X = np.delete(X, indexes, axis=0)\n",
        "y = np.delete(y, indexes, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((6468, 1, 26, 4000), (6468, 1))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Проверка размерностей\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QAEvqA1-1hg",
        "outputId": "286d686e-5099-4fbc-ad79-0b9ff27c454b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Относительное количество положительный записей:\n",
            "Вся выборка: [0.5]\n",
            "Обучающая выборка: [0.5036448]\n",
            "Валидационная выборка: [0.49149923]\n"
          ]
        }
      ],
      "source": [
        "# Разделение на train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)\n",
        "\n",
        "print('Относительное количество положительный записей:')\n",
        "print(f'Вся выборка: {sum(y) / len(y)}')\n",
        "print(f'Обучающая выборка: {sum(y_train) / len(y_train)}')\n",
        "print(f'Валидационная выборка: {sum(y_test) / len(y_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Заготовки\n",
        "# resource: https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples/blob/main/Vision_Transformer_with_tf2.ipynb\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Gaussian Error Linear Unit.\n",
        "    This is a smoother version of the RELU.\n",
        "    Original paper: https://arxiv.org/abs/1606.08415\n",
        "    Args:\n",
        "        x: float Tensor to perform activation.\n",
        "    Returns:\n",
        "        `x` with the GELU activation applied.\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.tanh(\n",
        "        (math.sqrt(2 / math.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "    return x * cdf\n",
        "\n",
        "\n",
        "def get_activation(identifier):\n",
        "    \"\"\"Maps a identifier to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
        "    It checks string first and if it is one of customized activation not in TF,\n",
        "    the corresponding activation will be returned. For non-customized activation\n",
        "    names and callable identifiers, always fallback to tf.keras.activations.get.\n",
        "    Args:\n",
        "        identifier: String name of the activation function or callable.\n",
        "    Returns:\n",
        "        A Python function corresponding to the activation function.\n",
        "    \"\"\"\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        name_to_fn = {\"gelu\": gelu}\n",
        "        identifier = str(identifier).lower()\n",
        "        if identifier in name_to_fn:\n",
        "            return tf.keras.activations.get(name_to_fn[identifier])\n",
        "    return tf.keras.activations.get(identifier)\n",
        "\n",
        "\n",
        "class Residual(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.fn(x) + x\n",
        "\n",
        "\n",
        "class PreNorm(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.fn = fn\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.fn(self.norm(x))\n",
        "\n",
        "\n",
        "class FeedForward(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = models.Sequential([layers.Dense(hidden_dim, activation=get_activation('gelu')),\n",
        "                                        layers.Dense(dim)])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, dim, heads = 8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = layers.Dense(dim * 3, use_bias=False)\n",
        "        self.to_out = layers.Dense(dim)\n",
        "\n",
        "        self.rearrange_qkv = Rearrange('b n (qkv h d) -> qkv b h n d', qkv = 3, h = self.heads)\n",
        "        self.rearrange_out = Rearrange('b h n d -> b n (h d)')\n",
        "\n",
        "    def call(self, x):\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = self.rearrange_qkv(qkv)\n",
        "        q = qkv[0]\n",
        "        k = qkv[1]\n",
        "        v = qkv[2]\n",
        "\n",
        "        dots = tf.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "        attn = tf.nn.softmax(dots,axis=-1)\n",
        "\n",
        "        out = tf.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = self.rearrange_out(out)\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for _ in range(depth):\n",
        "            layers.extend([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ])\n",
        "        self.net = tf.keras.Sequential(layers)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KnXvQjvnkMA2"
      },
      "outputs": [],
      "source": [
        "# Класс модели\n",
        "class ViTButForAIIJC(tf.keras.Model):\n",
        "    def __init__(self, *, image_size: tuple[int, int], patch_size: tuple[int, int], num_classes: int,\n",
        "                 dim: int, depth: int, heads: int, mlp_dim: int, channels: int):\n",
        "        \"\"\"Visual Transformer model for non-square images\n",
        "\n",
        "        Args:\n",
        "            image_size (tuple[int, int]): input vector sizes\n",
        "            patch_size (tuple[int, int]): path sizes\n",
        "            num_classes (int): number of classes and output shape\n",
        "            dim (int): embedding dims\n",
        "            depth (int): depth of transformer itself\n",
        "            heads (int): number of transforomer heads\n",
        "            mlp_dim (int): number of neurons in classification nn\n",
        "            channels (int): number channels in input\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert image_size[0] % patch_size[0] == 0 and image_size[1] % patch_size[1] == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1])\n",
        "        patch_dim = channels * patch_size[0] * patch_size[1]\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.dim = dim\n",
        "        self.pos_embedding = self.add_weight(name=\"position_embeddings\",\n",
        "                                             shape=[num_patches + 1, dim],\n",
        "                                             initializer=tf.keras.initializers.RandomNormal(),\n",
        "                                             dtype=tf.float32)\n",
        "        self.patch_to_embedding = layers.Dense(dim)\n",
        "        self.cls_token = self.add_weight(name=\"cls_token\",\n",
        "                                         shape=[1,\n",
        "                                                1,\n",
        "                                                dim],\n",
        "                                         initializer=tf.keras.initializers.RandomNormal(),\n",
        "                                         dtype=tf.float32)\n",
        "\n",
        "        self.rearrange = Rearrange('b c (w p1) (l p2) -> b (w l) (p1 p2 c)', p1=patch_size[0], p2=patch_size[1])\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = tf.identity\n",
        "\n",
        "        self.mlp_head = models.Sequential([\n",
        "            layers.Dense(mlp_dim, activation=get_activation('gelu')),\n",
        "            layers.Dense(num_classes, activation=('sigmoid' if num_classes == 1 else 'softmax'))\n",
        "        ])\n",
        "        self.mlp_head.name = 'classification_head'\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, fbank):\n",
        "        shapes = tf.shape(fbank)\n",
        "\n",
        "        x = self.rearrange(fbank)\n",
        "\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = tf.broadcast_to(self.cls_token,(shapes[0],1,self.dim))\n",
        "\n",
        "        x = tf.concat((cls_tokens, x), axis=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 26, 4000)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Проверка шейпов (читай)\n",
        "X[0].shape # (channels, image_size[0], image_size[1])\n",
        "# | | |\n",
        "# V V V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-26 23:32:32.001384: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
            "2024-08-26 23:32:32.001504: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2024-08-26 23:32:32.001513: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2024-08-26 23:32:32.001764: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-08-26 23:32:32.002284: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "# Конфиг для модели\n",
        "model = ViTButForAIIJC(\n",
        "    # Лучше не менять\n",
        "    image_size=(26, 4000),\n",
        "    patch_size=(2, 100),\n",
        "    num_classes=1,\n",
        "    channels=1,\n",
        "\n",
        "    # Можно крутить\n",
        "    dim=256,\n",
        "    depth=4,\n",
        "    heads=4,\n",
        "    mlp_dim=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C9n0HMQXkMA3"
      },
      "outputs": [],
      "source": [
        "# Конфиг для обучения\n",
        "epochs = 50\n",
        "batch_size=8\n",
        "optim = 'adam' # adam/sgd\n",
        "lr = 2e-5\n",
        "momentum = 0.8 # sgd only\n",
        "# early stop\n",
        "es_delta = 1e-3\n",
        "es_patience = 20\n",
        "# reducing lr\n",
        "red_lr_factor = 0.5\n",
        "red_lr_patience = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Применяем конфиг\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "if optim == 'adam':\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "else:\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
        "\n",
        "custom_callbacks = [\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        min_delta=es_delta,\n",
        "        patience=es_patience,\n",
        "        verbose=1,\n",
        "        mode='auto',\n",
        "        baseline=None,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    callbacks.ModelCheckpoint(\n",
        "        filepath='./saves/rnn_mfcc.weights.h5',\n",
        "        monitor='val_f1_score',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=red_lr_factor, patience=red_lr_patience, verbose=1)\n",
        "]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=[\n",
        "        'accuracy', \n",
        "        tf.keras.metrics.F1Score(average='macro', threshold=0.5),\n",
        "        tf.keras.metrics.Precision(),  # correct 1 / all predicted as 1\n",
        "        tf.keras.metrics.Recall()  # correct 1 / all 1\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Лес гоу 🥵\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;43;03m#    class_weight=class_weight  # У нас 50/50\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:282\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    275\u001b[0m     (\n\u001b[1;32m    276\u001b[0m         val_x,\n\u001b[1;32m    277\u001b[0m         val_y,\n\u001b[1;32m    278\u001b[0m         val_sample_weight,\n\u001b[1;32m    279\u001b[0m     ) \u001b[38;5;241m=\u001b[39m data_adapter_utils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(validation_data)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create an iterator that yields batches for one epoch.\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:629\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[0;32m--> 629\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[1;32m    631\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[1;32m    632\u001b[0m         dataset\n\u001b[1;32m    633\u001b[0m     )\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:638\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:236\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    234\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n\u001b[0;32m--> 236\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n\u001b[1;32m    239\u001b[0m options\u001b[38;5;241m.\u001b[39mexperimental_distribute\u001b[38;5;241m.\u001b[39mauto_shard_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    240\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAutoShardPolicy\u001b[38;5;241m.\u001b[39mDATA\n\u001b[1;32m    241\u001b[0m )\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:197\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[0;34m(indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m inputs \u001b[38;5;241m=\u001b[39m array_slicing\u001b[38;5;241m.\u001b[39mconvert_to_sliceable(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs, target_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    194\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mlists_to_tuples(inputs)\n\u001b[1;32m    196\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip(\n\u001b[0;32m--> 197\u001b[0m     (indices_dataset, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepeat())\n\u001b[1;32m    198\u001b[0m )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_one\u001b[39m(x):\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:741\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# from_tensors_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensors_op\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensors_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensors_op.py:23\u001b[0m, in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensors\u001b[39m(tensors, name):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensors_op.py:31\u001b[0m, in \u001b[0;36m_TensorDataset.__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `tf.data.Dataset.from_tensors` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure, element)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 134\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
            "File \u001b[0;32m/opt/miniconda3/envs/mfcc-cnn/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[1;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Лес гоу 🥵\n",
        "hist = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=epochs,\n",
        "    callbacks=custom_callbacks,\n",
        "    verbose=1,\n",
        "    validation_data=(X_test, y_test),\n",
        "    batch_size=batch_size\n",
        "#    class_weight=class_weight  # У нас 50/50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "CxyoeEVykMA4",
        "outputId": "0cb1373a-4723-4865-bb0e-169d587d85f4"
      },
      "outputs": [],
      "source": [
        "# Как прошли уроки?\n",
        "hist_df = pd.DataFrame(hist.history).drop(columns=['learning_rate'])\n",
        "hist_df.plot(figsize=(8,5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "UIz-dOrs-1hi",
        "outputId": "453d4698-9bd4-4c44-c79f-298cb703b679"
      },
      "outputs": [],
      "source": [
        "# Саммари\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkmInpbvpqgO"
      },
      "outputs": [],
      "source": [
        "# Загрзка весов\n",
        "model.load_weights('./saves/rnn_mfcc.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzcc8IRg-1hj",
        "outputId": "e0fc298c-b822-4123-8ef7-fcc72b129d71"
      },
      "outputs": [],
      "source": [
        "# Финальная оценочка\n",
        "print('На валидационной выборке')\n",
        "model.evaluate(X_test, y_test, return_dict=True)\n",
        "print('На всех данных')\n",
        "model.evaluate(X, y, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQir61AI-1hj"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gVa6Mi7-1hk"
      },
      "outputs": [],
      "source": [
        "# task = pd.read_csv(TASK_FILENAME, header=None, names=['audio', 'label'])\n",
        "# task['audio'] = TASK_DATA_PATH + \"/\" + task['audio']\n",
        "\n",
        "# task_x = task['audio']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq9teWIi-1hl"
      },
      "outputs": [],
      "source": [
        "# task_features_list = []\n",
        "\n",
        "# for audio in tqdm(task_x):\n",
        "#     task_features_list.append(preprocess_function(audio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# task_X = tf.keras.utils.pad_sequences(task_features_list, padding='post', maxlen=4000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgODkMfdetCJ"
      },
      "outputs": [],
      "source": [
        "# np.save(\"test\", task_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFPaNXg1pgOu"
      },
      "outputs": [],
      "source": [
        "task_X = np.load('test.npy').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gTzlvE_-1hl",
        "outputId": "b47d915d-93c4-4c3c-df9a-214da32885fb"
      },
      "outputs": [],
      "source": [
        "task_y = model.predict(task_X)\n",
        "task_y = np.round(task_y).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FLq71PjUtUV",
        "outputId": "a19b4c18-1f89-48cd-f7b8-5c31ad0470d3"
      },
      "outputs": [],
      "source": [
        "np.sum(task_y) / len(task_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3oaIdY7-1hl"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv(TASK_FILENAME, header=None, names=['audio', 'label'])\n",
        "submit['label'] = task_y\n",
        "submit.to_csv('submit.csv', header=False, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
